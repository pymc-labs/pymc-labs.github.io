<!doctype html><html lang="en">

    <head>
        

        <style media="screen">
            body {
                padding-top: 70px;
                padding-bottom: 70px;
            }

        </style>
        <!-- Required meta tags -->
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

        <!-- Bootstrap CSS -->
        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css"
            integrity="sha384-JcKb8q3iqJ61gNV9KGb8thSsNjpSL0n8PARn9HuZOnIxN0hoP+VmmDGMN5t9UJ0Z" crossorigin="anonymous">
        <link rel="stylesheet" href="../../static/css/custom_style.css?h=16f93eb7">
        <link rel="stylesheet" href="../../static/css/table_style.css?h=c677f945">

        <!-- Highlight.js for syntax highlighting -->
        <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.7.2/styles/default.min.css">


        <!-- Extra meta tags: social site cards, browser icons... -->
        <meta name="theme-color" content="#ffffff">
        <link rel="shortcut icon" href="../../static/favicon.ico?h=d935d59e">
        <link rel="apple-touch-icon" sizes="180x180" href="../../static/apple-touch-icon.png?h=2bad941d">
        <link rel="icon" type="image/png" sizes="32x32" href="../../static/favicon-32x32.png?h=1673bb68">
        <link rel="icon" type="image/png" sizes="16x16" href="../../static/favicon-16x16.png?h=089e66cb">

        <title>How to use JAX ODEs and Neural Networks in PyMC - PyMC Labs</title>
        <meta name="twitter:card" content="summary">
        <meta property="og:url" content="https://pymc-labs.github.io/blog-posts/jax-functions-in-pymc-3-quick-examples/" />
        <meta property="og:type" content="website" />
        <link rel="canonical" href="">
        <meta property="og:title" content="How to use JAX ODEs and Neural Networks in PyMC - PyMC Labs" />
        <meta property="og:description" content="Or anything else, really" />
        <meta property="og:image" content="https://pymc-labs.github.io/blog-posts/jax-functions-in-pymc-3-quick-examples/cover.png" />
        <meta name="description" content="We are a Bayesian consulting firm specializing in data analysis and predictive modeling. Contact us today to learn how we can help your business.">
        <meta name="keywords" content="Bayesian consulting, data analysis, predictive modeling">

        <!-- Highlight.js for syntax highlighting -->
        <!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.0.0/styles/default.min.css"> -->
        <!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.0.0/highlight.min.js"></script> -->
        <!-- <script>hljs.highlightAll();</script> -->

        <!-- From: https://github.com/lektor/lektor-markdown-highlighter -->
        <!-- We use this to do syntax highlighting -->
        <link rel="stylesheet" href="../../static/pygments.css">
        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-F3RDLH8R8X"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-F3RDLH8R8X');
        </script>
        
<script src="../../static/scripts/toggle_code.js?h=3a00c72f" defer></script>

    </head>

    <body>
        <!-- Navigation -->
        <nav class="navbar navbar-expand-lg navbar-light bg-light fixed-top">
            <div class="container">
                <!-- <a class="navbar-brand" href="/">PyMC Labs</a> -->
                <a class="navbar-brand" href="/"><img alt="logo" loading="eager" width="88" height="70" title="logo" class="navbar-logo"
                        src="../../static/images/pymc-labs-logo.png?h=999c3177'"></a>
                <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarTop"
                    aria-controls="navbarTop" aria-expanded="false" aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon"></span>
                </button>
                <div class="collapse navbar-collapse" id="navbarTop">
                    <ul class="navbar-nav ml-auto">
                        
                        <li class="nav-item">
                            <a class="nav-link" href="/what-we-do"><i class="fa fa-info-circle"
                                    aria-hidden="true"></i>
                                What we do</a>
                        </li>
                        
                        <li class="nav-item">
                            <a class="nav-link" href="/products"><i class="fa fa-shopping-cart"
                                    aria-hidden="true"></i>
                                Products</a>
                        </li>
                        
                        <li class="nav-item">
                            <a class="nav-link" href="/team"><i class="fa fa-user-friends"
                                    aria-hidden="true"></i>
                                Team</a>
                        </li>
                        
                        <li class="nav-item">
                            <a class="nav-link" href="/clients"><i class="fa fa-microphone"
                                    aria-hidden="true"></i>
                                Clients</a>
                        </li>
                        
                        <li class="nav-item">
                            <a class="nav-link" href="/workshops"><i class="fa fa-chalkboard-teacher"
                                    aria-hidden="true"></i>
                                Workshops</a>
                        </li>
                        
                        <li class="nav-item">
                            <a class="nav-link" href="/blog-posts"><i class="fa fa-book-open"
                                    aria-hidden="true"></i>
                                Blog</a>
                        </li>
                        
                    </ul>
                </div>
            </div>
        </nav>
        
        <div class="container">
            

<div class="row">
    <div class="col-md-2"></div>
    <div class="col-md-8 blogpost">
        <h2 class="font-roboto">How to use JAX ODEs and Neural Networks in PyMC</h2>
        
        <p class="mb-2 text-muted">Or anything else, really</p>
        
        <hr>
        <div class="row">
            <div class="col-md-6 author_name">
                <small class="text-muted">AUTHORED BY</small>
                <p class="font-bold">
                    



    



    
        
            Ricardo Vieira
        
    
        
            and Adrian Seyboldt
        
    



                </p>
            </div>
            <div class="col-md-6 author_date">
                <!-- <p>2023-01-03</p> -->
                
<small class="text-muted">DATE</small>
<p class="font-lighter">2023-01-03</p>

<!--<div class="cover-blogposts"><img src="../../static/images/blog_post/cover.jpg?h=653e9b57"></div>-->

            </div>
        
            
                <div class="blog-cover-container">
                    <img loading="lazy" title="cover image" alt="" class="cover-blogposts" src="cover.png">
                </div>
            
	    </div>
        <hr> <p>PyMC strength comes from its expressiveness.
If you have a data-generating process and want to infer parameters of interest,
all you need to do is write it down, choose some priors and let it sample.</p>
<p>Sometimes this is easier said than done, especially the "write it down" part.
With Python's rich ecosystem,
it's often the case that you already have a generative function,
but it's written in another framework, and you would like to use it in PyMC.
Thanks to the highly composable nature of the PyMC backend, this is simple.
Even simpler if that framework can also provide you gradients for free!</p>
<p>In this blog post,
we show how you can reuse code from another popular auto-diff framework,
JAX, directly in PyMC.</p>
<p>We will start with a dummy example by simply wrapping
a pure function that already exists under <code>pymc.math</code>,
and then show two real examples:
reusing an ODE Solver from the Diffrax library
and a CNN from the Flax library.</p>
<p>This blog post won't explain in detail why we do things the way they are shown,
but will only show you how to do it.
If you want to have a better understanding, you should check the PyMC example
<a href="https://www.pymc.io/projects/examples/en/latest/case_studies/wrapping_jax_function.html">How to wrap a JAX function for use in PyMC</a>
and the relevant PyTensor <a href="https://pytensor.readthedocs.io/en/latest/extending/op.html">documentation of <code>Op</code></a>.</p>
<p>Without further ado, let's import some stuff.</p>
<div class="hll"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">pytensor</span>
<span class="kn">import</span> <span class="nn">pytensor.tensor</span> <span class="k">as</span> <span class="nn">pt</span>
<span class="kn">from</span> <span class="nn">pytensor.graph</span> <span class="kn">import</span> <span class="n">Apply</span><span class="p">,</span> <span class="n">Op</span>
<span class="kn">from</span> <span class="nn">pytensor.link.jax.dispatch</span> <span class="kn">import</span> <span class="n">jax_funcify</span>

<span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>

<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="nn">pm</span>
<span class="kn">import</span> <span class="nn">pymc.sampling.jax</span>
</pre></div>
<h2 id="wrapping-a-pure-jax-function">Wrapping a pure JAX function</h2><p>In this first example,
we will wrap the <code>jax.numpy.exp</code> function so you can use it in PyMC models.
This is purely demonstrative, as you could use <code>pymc.math.exp</code>.</p>
<p>We first create a function that encapsulates the operation (or series of operations)
that we care about.
We also save the jitted function into a variable.</p>
<div class="hll"><pre><span></span><span class="k">def</span> <span class="nf">custom_op_jax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">jitted_custom_op_jax</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">custom_op_jax</span><span class="p">)</span>
</pre></div>
<p>JAX's <code>jit</code> function accepts a function and returns a function,
meaning that we can call on <code>jitted_custom_op_jax</code>:</p>
<div class="hll"><pre><span></span><span class="n">jitted_custom_op_jax</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
</pre></div>
<p>We then create the function that computes
the vector-jacobian product (vjp) needed for PyTensor gradients.
JAX <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.vjp.html">vjp</a>
takes as inputs a computational graph, expressed as a function, and its inputs.
It returns the evaluated graph, which we don't need,
and a partial function that computes the vjp,
given the output gradients,
which is what we need for PyTensor.</p>
<div class="hll"><pre><span></span><span class="k">def</span> <span class="nf">vjp_custom_op_jax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">gz</span><span class="p">):</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">vjp_fn</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vjp</span><span class="p">(</span><span class="n">custom_op_jax</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">vjp_fn</span><span class="p">(</span><span class="n">gz</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">jitted_vjp_custom_op_jax</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">vjp_custom_op_jax</span><span class="p">)</span>
</pre></div>
<p>Now for the meaty part!
We create two PyTensor Ops,
one for the operation we care about
and another for the vjp of that operation.
This is how we can glue external code into PyMC's backend.
(Note here: It's a bit verbose, but nothing too complicated.)</p>
<p>Here's what's happening below.
We subclass from the <code>Op</code> class
and implement 3 methods: <code>make_node</code>, <code>perform</code> and <code>grad</code>.
For the vjp we need to implement only the first two.</p>
<div class="hll"><pre><span></span><span class="c1"># The CustomOp needs `make_node`, `perform` and `grad`.</span>
<span class="k">class</span> <span class="nc">CustomOp</span><span class="p">(</span><span class="n">Op</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">make_node</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Create a PyTensor node specifying the number and type of inputs and outputs</span>

        <span class="c1"># We convert the input into a PyTensor tensor variable</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">pt</span><span class="o">.</span><span class="n">as_tensor_variable</span><span class="p">(</span><span class="n">x</span><span class="p">)]</span>
        <span class="c1"># Output has the same type and shape as `x`</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">type</span><span class="p">()]</span>
        <span class="k">return</span> <span class="n">Apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">perform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
        <span class="c1"># Evaluate the Op result for a specific numerical input</span>

        <span class="c1"># The inputs are always wrapped in a list</span>
        <span class="p">(</span><span class="n">x</span><span class="p">,)</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">jitted_custom_op_jax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># The results should be assigned inplace to the nested list</span>
        <span class="c1"># of outputs provided by PyTensor. If you have multiple</span>
        <span class="c1"># outputs and results, you should assign each at outputs[i][0]</span>
        <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float64&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">output_gradients</span><span class="p">):</span>
        <span class="c1"># Create a PyTensor expression of the gradient</span>
        <span class="p">(</span><span class="n">x</span><span class="p">,)</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="p">(</span><span class="n">gz</span><span class="p">,)</span> <span class="o">=</span> <span class="n">output_gradients</span>
        <span class="c1"># We reference the VJP Op created below, which encapsulates</span>
        <span class="c1"># the gradient operation</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">vjp_custom_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">gz</span><span class="p">)]</span>


<span class="k">class</span> <span class="nc">VJPCustomOp</span><span class="p">(</span><span class="n">Op</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">make_node</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">gz</span><span class="p">):</span>
        <span class="c1"># Make sure the two inputs are tensor variables</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">pt</span><span class="o">.</span><span class="n">as_tensor_variable</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">pt</span><span class="o">.</span><span class="n">as_tensor_variable</span><span class="p">(</span><span class="n">gz</span><span class="p">)]</span>
        <span class="c1"># Output has the shape type and shape as the first input</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">type</span><span class="p">()]</span>
        <span class="k">return</span> <span class="n">Apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">perform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
        <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">gz</span><span class="p">)</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">jitted_vjp_custom_op_jax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">gz</span><span class="p">)</span>
        <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float64&quot;</span><span class="p">)</span>

<span class="c1"># Instantiate the Ops</span>
<span class="n">custom_op</span> <span class="o">=</span> <span class="n">CustomOp</span><span class="p">()</span>
<span class="n">vjp_custom_op</span> <span class="o">=</span> <span class="n">VJPCustomOp</span><span class="p">()</span>
</pre></div>
<p>How do we know that we've implemented the <code>Op</code>s correctly?
To do that, we can use the pytensor <code>verify_grad</code> utility:</p>
<div class="hll"><pre><span></span><span class="n">pytensor</span><span class="o">.</span><span class="n">gradient</span><span class="o">.</span><span class="n">verify_grad</span><span class="p">(</span><span class="n">custom_op</span><span class="p">,</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float64&quot;</span><span class="p">),),</span> <span class="n">rng</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">())</span>
</pre></div>
<p>It didn't raise an <code>Error</code>, so we're clear!
Now we can use our wrapped Op directly with PyMC models:</p>
<div class="hll"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">custom_op</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c1"># HERE IS WHERE WE USE THE CUSTOM OP!</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;z&quot;</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
</pre></div>
<p>PyMC provides <code>model_to_graphviz</code> to visualize the model graph:</p>
<div class="hll"><pre><span></span><span class="n">pm</span><span class="o">.</span><span class="n">model_to_graphviz</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
<p><img src="output_15_0.svg" alt="svg"></p>
<p>Part of verifying that the <code>Op</code>'s API works with PyMC
is in evaluating the model's <code>logp</code> and <code>dlogp</code>.
First, the <code>logp</code> function:</p>
<div class="hll"><pre><span></span><span class="c1"># Compute the logp at the initial point</span>
<span class="n">ip</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">initial_point</span><span class="p">()</span>
<span class="n">logp_fn</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">compile_fn</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">logp</span><span class="p">(</span><span class="nb">sum</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
<span class="n">logp_fn</span><span class="p">(</span><span class="n">ip</span><span class="p">)</span>
</pre></div>
<div class="hll"><pre><span></span><span class="p">[</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.91893853</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.91893853</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.91893853</span><span class="p">]),</span>
 <span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.91893853</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.41893853</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.41893853</span><span class="p">])]</span>
</pre></div>
<p>And now the <code>dlogp</code> function:</p>
<div class="hll"><pre><span></span><span class="n">dlogp_fn</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">compile_fn</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">dlogp</span><span class="p">())</span>
<span class="n">dlogp_fn</span><span class="p">(</span><span class="n">ip</span><span class="p">)</span>
</pre></div>
<div class="hll"><pre><span></span><span class="n">array</span><span class="p">([</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">])</span>
</pre></div>
<p>The numerical values of the evaluated <code>logp</code> and <code>dlogp</code> are correct in both cases.</p>
<p>If we want to use PyTensor's JAX backend,
we have to, somewhat paradoxically, tell PyTensor how to convert our Ops to JAX code.
PyTensor does not know it was JAX code to begin with!
Fortunately, this is pretty simple by simply returning the original functions.</p>
<p>Note that we don't return the jitted functions,
because we want PyTensor to use JAX to jit the whole JAX graph together.</p>
<div class="hll"><pre><span></span><span class="nd">@jax_funcify</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">CustomOp</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">custom_op_jax_funcify</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">custom_op_jax</span>

<span class="nd">@jax_funcify</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">VJPCustomOp</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">vjp_custom_op_jax_funcify</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">vjp_custom_op_jax</span>
</pre></div>
<p>Now we can compile to the JAX backend and get the same results!
First with the <code>logp</code>:</p>
<div class="hll"><pre><span></span><span class="c1"># Now using the JAX backend</span>
<span class="n">ip</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">initial_point</span><span class="p">()</span>
<span class="n">logp_fn</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">compile_fn</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">logp</span><span class="p">(</span><span class="nb">sum</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;JAX&quot;</span><span class="p">)</span>
<span class="n">logp_fn</span><span class="p">(</span><span class="n">ip</span><span class="p">)</span>
</pre></div>
<div class="hll"><pre><span></span><span class="p">[</span><span class="n">DeviceArray</span><span class="p">([</span><span class="o">-</span><span class="mf">0.91893853</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.91893853</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.91893853</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float64</span><span class="p">),</span>
 <span class="n">DeviceArray</span><span class="p">([</span><span class="o">-</span><span class="mf">0.91893853</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.41893853</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.41893853</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float64</span><span class="p">)]</span>
</pre></div>
<p>And now with the <code>dlogp</code>:</p>
<div class="hll"><pre><span></span><span class="n">dlogp_fn</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">compile_fn</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">dlogp</span><span class="p">(),</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;JAX&quot;</span><span class="p">)</span>
<span class="n">dlogp_fn</span><span class="p">(</span><span class="n">ip</span><span class="p">)</span>
</pre></div>
<div class="hll"><pre><span></span><span class="n">DeviceArray</span><span class="p">([[</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float64</span><span class="p">)</span>
</pre></div>
<h2 id="wrapping-a-diffrax-ode-solver">Wrapping a Diffrax ODE Solver</h2><p>Let's move on to a more complicated situation.
We will wrap an ODE solver from the Diffrax library in this second example.
It will be a straightforward example with a single variable parameter:
The initial point <code>y0</code>.
First, we import <code>diffrax</code>:</p>
<div class="hll"><pre><span></span><span class="kn">import</span> <span class="nn">diffrax</span>
</pre></div>
<p>Then we set up a simple ODE.</p>
<div class="hll"><pre><span></span><span class="n">vector_field</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">t</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">args</span><span class="p">:</span> <span class="o">-</span><span class="n">y</span>
<span class="n">term</span> <span class="o">=</span> <span class="n">diffrax</span><span class="o">.</span><span class="n">ODETerm</span><span class="p">(</span><span class="n">vector_field</span><span class="p">)</span>
<span class="n">solver</span> <span class="o">=</span> <span class="n">diffrax</span><span class="o">.</span><span class="n">Dopri5</span><span class="p">()</span>
<span class="n">saveat</span> <span class="o">=</span> <span class="n">diffrax</span><span class="o">.</span><span class="n">SaveAt</span><span class="p">(</span><span class="n">ts</span><span class="o">=</span><span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">])</span>
<span class="n">stepsize_controller</span> <span class="o">=</span> <span class="n">diffrax</span><span class="o">.</span><span class="n">PIDController</span><span class="p">(</span><span class="n">rtol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>

<span class="n">sol</span> <span class="o">=</span> <span class="n">diffrax</span><span class="o">.</span><span class="n">diffeqsolve</span><span class="p">(</span><span class="n">term</span><span class="p">,</span> <span class="n">solver</span><span class="p">,</span> <span class="n">t0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">t1</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">dt0</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">y0</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">saveat</span><span class="o">=</span><span class="n">saveat</span><span class="p">,</span>
                  <span class="n">stepsize_controller</span><span class="o">=</span><span class="n">stepsize_controller</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">sol</span><span class="o">.</span><span class="n">ts</span><span class="p">)</span>  <span class="c1"># DeviceArray([0.   , 1.   , 2.   , 3.    ])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sol</span><span class="o">.</span><span class="n">ys</span><span class="p">)</span>  <span class="c1"># DeviceArray([1.   , 0.368, 0.135, 0.0498])</span>
</pre></div>
<div class="hll"><pre><span></span><span class="n">DeviceArray</span><span class="p">([</span><span class="mf">0.</span>   <span class="p">,</span> <span class="mf">1.</span>   <span class="p">,</span> <span class="mf">2.</span>   <span class="p">,</span> <span class="mf">3.</span>    <span class="p">])</span>
<span class="n">DeviceArray</span><span class="p">([</span><span class="mf">1.</span>   <span class="p">,</span> <span class="mf">0.368</span><span class="p">,</span> <span class="mf">0.135</span><span class="p">,</span> <span class="mf">0.0498</span><span class="p">])</span>
</pre></div>
<p>For those who are not familiar with ODEs,
the <code>vector_field</code> is the derivative of the function <code>y</code> with respect to <code>t</code>,
and the <code>term</code> is the ODE itself.
The <code>solver</code> is the method used to solve the ODE;
the <code>saveat</code> is the collection of times at which we want to save the solution;
and the <code>stepsize_controller</code> is used to control the step size of the solver.
Finally, <code>sol</code> is the solution to the ODE, evaluated at the <code>saveat</code> points.</p>
<p>From this point onward,
the rest of the code should look very similar to what we did above.</p>
<p>Firstly, we need a JAX function that we will wrap.
Our function will return the solutions for <code>ys</code>, given a starting point <code>y0</code>.
The other parameters will be constant for this example,
but they could also be variables in a more complex <code>Op</code>.</p>
<div class="hll"><pre><span></span><span class="k">def</span> <span class="nf">sol_op_jax</span><span class="p">(</span><span class="n">y0</span><span class="p">):</span>
    <span class="n">sol</span> <span class="o">=</span> <span class="n">diffrax</span><span class="o">.</span><span class="n">diffeqsolve</span><span class="p">(</span>
        <span class="n">term</span><span class="p">,</span>
        <span class="n">solver</span><span class="p">,</span>
        <span class="n">t0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">t1</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="n">dt0</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="n">y0</span><span class="o">=</span><span class="n">y0</span><span class="p">,</span>
        <span class="n">saveat</span><span class="o">=</span><span class="n">saveat</span><span class="p">,</span>
        <span class="n">stepsize_controller</span><span class="o">=</span><span class="n">stepsize_controller</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">sol</span><span class="o">.</span><span class="n">ys</span>

<span class="n">jitted_sol_op_jax</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">sol_op_jax</span><span class="p">)</span>
</pre></div>
<p>Then, we define the vjp function.</p>
<div class="hll"><pre><span></span><span class="k">def</span> <span class="nf">vjp_sol_op_jax</span><span class="p">(</span><span class="n">y0</span><span class="p">,</span> <span class="n">gz</span><span class="p">):</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">vjp_fn</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vjp</span><span class="p">(</span><span class="n">sol_op_jax</span><span class="p">,</span> <span class="n">y0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">vjp_fn</span><span class="p">(</span><span class="n">gz</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">jitted_vjp_sol_op_jax</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">vjp_sol_op_jax</span><span class="p">)</span>
</pre></div>
<p>After that, we define the <code>Op</code> and <code>VJPOp</code> classes for the ODE problem:</p>
<div class="hll"><pre><span></span><span class="k">class</span> <span class="nc">SolOp</span><span class="p">(</span><span class="n">Op</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">make_node</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y0</span><span class="p">):</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">pt</span><span class="o">.</span><span class="n">as_tensor_variable</span><span class="p">(</span><span class="n">y0</span><span class="p">)]</span>
        <span class="c1"># Assume the output to always be a float64 vector</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">pt</span><span class="o">.</span><span class="n">vector</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float64&quot;</span><span class="p">)]</span>
        <span class="k">return</span> <span class="n">Apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">perform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
        <span class="p">(</span><span class="n">y0</span><span class="p">,)</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">jitted_sol_op_jax</span><span class="p">(</span><span class="n">y0</span><span class="p">)</span>
        <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float64&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">output_gradients</span><span class="p">):</span>
        <span class="p">(</span><span class="n">y0</span><span class="p">,)</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="p">(</span><span class="n">gz</span><span class="p">,)</span> <span class="o">=</span> <span class="n">output_gradients</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">vjp_sol_op</span><span class="p">(</span><span class="n">y0</span><span class="p">,</span> <span class="n">gz</span><span class="p">)]</span>


<span class="k">class</span> <span class="nc">VJPSolOp</span><span class="p">(</span><span class="n">Op</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">make_node</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y0</span><span class="p">,</span> <span class="n">gz</span><span class="p">):</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">pt</span><span class="o">.</span><span class="n">as_tensor_variable</span><span class="p">(</span><span class="n">y0</span><span class="p">),</span> <span class="n">pt</span><span class="o">.</span><span class="n">as_tensor_variable</span><span class="p">(</span><span class="n">gz</span><span class="p">)]</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">type</span><span class="p">()]</span>
        <span class="k">return</span> <span class="n">Apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">perform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
        <span class="p">(</span><span class="n">y0</span><span class="p">,</span> <span class="n">gz</span><span class="p">)</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">jitted_vjp_sol_op_jax</span><span class="p">(</span><span class="n">y0</span><span class="p">,</span> <span class="n">gz</span><span class="p">)</span>
        <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float64&quot;</span><span class="p">)</span>

<span class="n">sol_op</span> <span class="o">=</span> <span class="n">SolOp</span><span class="p">()</span>
<span class="n">vjp_sol_op</span> <span class="o">=</span> <span class="n">VJPSolOp</span><span class="p">()</span>
</pre></div>
<div class="hll"><pre><span></span><span class="n">pytensor</span><span class="o">.</span><span class="n">gradient</span><span class="o">.</span><span class="n">verify_grad</span><span class="p">(</span><span class="n">sol_op</span><span class="p">,</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">3.0</span><span class="p">),),</span> <span class="n">rng</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">())</span>
</pre></div>
<p>And with no errors, we go on to register the JAX-ified versions of the <code>Op</code> and <code>VJPOp</code>:</p>
<div class="hll"><pre><span></span><span class="nd">@jax_funcify</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">SolOp</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">sol_op_jax_funcify</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">sol_op_jax</span>

<span class="nd">@jax_funcify</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">VJPSolOp</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">vjp_sol_op_jax_funcify</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">vjp_sol_op_jax</span>
</pre></div>
<p>Finally, we can use the <code>Op</code> in a model,
this time to infer what the initial value of the ODE was from observed data:</p>
<div class="hll"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">y0</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;y0&quot;</span><span class="p">)</span>
    <span class="n">ys</span> <span class="o">=</span> <span class="n">sol_op</span><span class="p">(</span><span class="n">y0</span><span class="p">)</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s2">&quot;noise&quot;</span><span class="p">)</span>
    <span class="n">llike</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;llike&quot;</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">noise</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.367</span><span class="p">,</span> <span class="mf">0.135</span><span class="p">,</span> <span class="mf">0.049</span><span class="p">])</span>
</pre></div>
<p>As always, we can inspect the model's structure to make sure it is correct:</p>
<div class="hll"><pre><span></span><span class="n">pm</span><span class="o">.</span><span class="n">model_to_graphviz</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
<p><img src="output_35_0.svg" alt="svg"></p>
<p>And finally, we can verify that the model's <code>logp</code> and <code>dlogp</code> functions execute.
Firstly, without JAX mode:</p>
<div class="hll"><pre><span></span><span class="n">ip</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">initial_point</span><span class="p">()</span>
<span class="n">logp_fn</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">compile_fn</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">logp</span><span class="p">(</span><span class="nb">sum</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
<span class="n">logp_fn</span><span class="p">(</span><span class="n">ip</span><span class="p">)</span>
</pre></div>
<div class="hll"><pre><span></span><span class="p">[</span><span class="n">array</span><span class="p">(</span><span class="o">-</span><span class="mf">0.91893853</span><span class="p">),</span>
 <span class="n">array</span><span class="p">(</span><span class="o">-</span><span class="mf">0.72579135</span><span class="p">),</span>
 <span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">1.41893853</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.98628303</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.92805103</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.92013903</span><span class="p">])]</span>
</pre></div>
<p>And then with JAX mode:</p>
<div class="hll"><pre><span></span><span class="n">logp_fn</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">compile_fn</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">logp</span><span class="p">(</span><span class="nb">sum</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;JAX&quot;</span><span class="p">)</span>
<span class="n">logp_fn</span><span class="p">(</span><span class="n">ip</span><span class="p">)</span>
</pre></div>
<div class="hll"><pre><span></span><span class="p">[</span><span class="n">DeviceArray</span><span class="p">(</span><span class="o">-</span><span class="mf">0.91893853</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float64</span><span class="p">),</span>
 <span class="n">DeviceArray</span><span class="p">(</span><span class="o">-</span><span class="mf">0.72579135</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float64</span><span class="p">),</span>
 <span class="n">DeviceArray</span><span class="p">([</span><span class="o">-</span><span class="mf">1.41893853</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.98628303</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.92805103</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.92013903</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float64</span><span class="p">)]</span>
</pre></div>
<p>And then the <code>dlogp</code> functions in both non-JAX and JAX mode:</p>
<div class="hll"><pre><span></span><span class="n">dlogp_fn</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">compile_fn</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">dlogp</span><span class="p">())</span>
<span class="n">dlogp_fn</span><span class="p">(</span><span class="n">ip</span><span class="p">)</span>
</pre></div>
<div class="hll"><pre><span></span><span class="n">array</span><span class="p">([</span> <span class="mf">1.15494948</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.844685</span>  <span class="p">])</span>
</pre></div>
<div class="hll"><pre><span></span><span class="n">dlogp_fn</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">compile_fn</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">dlogp</span><span class="p">(),</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;JAX&quot;</span><span class="p">)</span>
<span class="n">dlogp_fn</span><span class="p">(</span><span class="n">ip</span><span class="p">)</span>
</pre></div>
<div class="hll"><pre><span></span><span class="n">DeviceArray</span><span class="p">([</span> <span class="mf">1.15494948</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.844685</span>  <span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float64</span><span class="p">)</span>
</pre></div>
<h2 id="wrapping-a-flax-neural-network">Wrapping a Flax neural network</h2><p>Our final example will be encapsulating a Neural Network
built with the Flax library.
In this example, we will skip the gradient implementation.
As discussed below, you don't need to implement it
if you defer the gradient transformation to JAX,
as PyMC does when using <code>sampling.jax</code>.</p>
<p>In this problem setup, we will be training a CNN
to predict digit identity in a given MNIST dataset image.
We will make use of <code>tensorflow_datasets</code> to get access to the MNIST dataset:</p>
<div class="hll"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow_datasets</span> <span class="k">as</span> <span class="nn">tfds</span>

<span class="kn">from</span> <span class="nn">flax</span> <span class="kn">import</span> <span class="n">linen</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">flax.core</span> <span class="kn">import</span> <span class="n">freeze</span>

<span class="k">def</span> <span class="nf">get_datasets</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Load MNIST train and test datasets into memory.&quot;&quot;&quot;</span>

    <span class="n">ds_builder</span> <span class="o">=</span> <span class="n">tfds</span><span class="o">.</span><span class="n">builder</span><span class="p">(</span><span class="s1">&#39;mnist&#39;</span><span class="p">)</span>
    <span class="n">ds_builder</span><span class="o">.</span><span class="n">download_and_prepare</span><span class="p">()</span>
    <span class="n">train_ds</span> <span class="o">=</span> <span class="n">tfds</span><span class="o">.</span><span class="n">as_numpy</span><span class="p">(</span><span class="n">ds_builder</span><span class="o">.</span><span class="n">as_dataset</span><span class="p">(</span><span class="n">split</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">test_ds</span> <span class="o">=</span> <span class="n">tfds</span><span class="o">.</span><span class="n">as_numpy</span><span class="p">(</span><span class="n">ds_builder</span><span class="o">.</span><span class="n">as_dataset</span><span class="p">(</span><span class="n">split</span><span class="o">=</span><span class="s1">&#39;test&#39;</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">train_ds</span><span class="p">[</span><span class="s1">&#39;image&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">train_ds</span><span class="p">[</span><span class="s1">&#39;image&#39;</span><span class="p">])</span> <span class="o">/</span> <span class="mf">255.</span>
    <span class="n">test_ds</span><span class="p">[</span><span class="s1">&#39;image&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">test_ds</span><span class="p">[</span><span class="s1">&#39;image&#39;</span><span class="p">])</span> <span class="o">/</span> <span class="mf">255.</span>
    <span class="k">return</span> <span class="n">train_ds</span><span class="p">,</span> <span class="n">test_ds</span>

<span class="n">train</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">get_datasets</span><span class="p">()</span>
<span class="n">train</span><span class="p">[</span><span class="s2">&quot;image&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="s2">&quot;image&quot;</span><span class="p">][:</span><span class="mi">1_000</span><span class="p">]</span>
<span class="n">train</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">][:</span><span class="mi">1_000</span><span class="p">]</span>
</pre></div>
<p>We can inspect the dataset to figure out its dimensions:</p>
<div class="hll"><pre><span></span><span class="n">train</span><span class="p">[</span><span class="s2">&quot;image&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
<div class="hll"><pre><span></span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch_size, height, width, channels)</span>
</pre></div>
<p>Here, we selected 1,000 images, each of which is 28x28 pixels, with 1 channel</p>
<p>Let's see what one of those images looks like:</p>
<div class="hll"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">train</span><span class="p">[</span><span class="s2">&quot;image&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">train</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]);</span>
</pre></div>
<p><img src="output_46_0.png" alt="png"></p>
<p>Now, we will implement a simple Convolution Neural Network (CNN)
using the very user-friendly Flax library.
(It has an API that is very, very close in spirit to PyTorch.)</p>
<div class="hll"><pre><span></span><span class="k">class</span> <span class="nc">CNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A simple CNN model.&quot;&quot;&quot;</span>

    <span class="nd">@nn</span><span class="o">.</span><span class="n">compact</span>
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Convolution layer</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">avg_pool</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">window_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

        <span class="c1"># Convolution layer</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">avg_pool</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">window_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

        <span class="c1"># Dense layer</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># flatten</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="mi">256</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Output layer</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="mi">10</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>
</pre></div>
<p>The exact structure of the model is unimportant here;
what <em>is</em> important, though, is that the model is a <em>callable</em>.</p>
<p>Let's initialize the CNN and iterate over the layers
to get an idea of the number of parameters</p>
<div class="hll"><pre><span></span><span class="n">cnn</span> <span class="o">=</span> <span class="n">CNN</span><span class="p">()</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">rng</span><span class="p">,</span> <span class="n">init_rng</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">rng</span><span class="p">)</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">cnn</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>

<span class="n">n_params</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">params</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
    <span class="n">n_params</span> <span class="o">+=</span> <span class="n">layer</span><span class="p">[</span><span class="s2">&quot;kernel&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">size</span> <span class="o">+</span> <span class="n">layer</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;bias&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(()))</span><span class="o">.</span><span class="n">size</span>
<span class="n">n_params</span>
</pre></div>
<div class="hll"><pre><span></span><span class="mi">824458</span>
</pre></div>
<p>This model has a lot of parameters,
many more than most of the classical statistical estimation models will have.</p>
<p>We can evaluate the forward pass of the network by calling <code>cnn.apply</code>.
<em>This is the function we want to wrap for use in PyMC.</em></p>
<div class="hll"><pre><span></span><span class="n">cnn</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">train</span><span class="p">[</span><span class="s2">&quot;image&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
<div class="hll"><pre><span></span><span class="n">DeviceArray</span><span class="p">([[</span> <span class="mf">0.11332617</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.06580747</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.06869425</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.02406035</span><span class="p">,</span>
              <span class="o">-</span><span class="mf">0.05488511</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.00442111</span><span class="p">,</span>  <span class="mf">0.05316056</span><span class="p">,</span>  <span class="mf">0.1178513</span> <span class="p">,</span>
               <span class="mf">0.10901824</span><span class="p">,</span>  <span class="mf">0.09090584</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span>
</pre></div>
<p>We want to pass the weights of each kernel as vanilla arrays,
but FLAX requires them to be in a tree structure for evaluation.
This requires using some utilities, but it's otherwise straightforward.
Note that this is specific to Flax, of course.</p>
<div class="hll"><pre><span></span><span class="n">treedef</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_util</span><span class="o">.</span><span class="n">tree_structure</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">cnn_op_jax</span><span class="p">(</span><span class="n">flat_params</span><span class="p">,</span> <span class="n">images</span><span class="p">):</span>
    <span class="n">unflat_params</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_util</span><span class="o">.</span><span class="n">tree_unflatten</span><span class="p">(</span><span class="n">treedef</span><span class="p">,</span> <span class="n">flat_params</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cnn</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">unflat_params</span><span class="p">,</span> <span class="n">images</span><span class="p">)</span>

<span class="n">jitted_cnn_op_jax</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">cnn_op_jax</span><span class="p">)</span>
</pre></div>
<p>If you are feeling a bit confused
because of the presence of "unflattened" and "flattened" parameters,
don't worry: it's just a technicality we need to deal with now.
What's worth noting here is that the CNN's forward pass
is wrapped in a JAX function that will be wrapped in a PyTensor Op,
<em>just as we had done before</em>.</p>
<p>Now, let's create the CNN Op.
Note that we don't implement the gradient method!</p>
<div class="hll"><pre><span></span><span class="k">class</span> <span class="nc">CNNOp</span><span class="p">(</span><span class="n">Op</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">make_node</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
        <span class="c1"># Convert our inputs to symbolic variables</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">pt</span><span class="o">.</span><span class="n">as_tensor_variable</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span> <span class="k">for</span> <span class="n">inp</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">]</span>
        <span class="c1"># Assume the output to always be a float64 matrix</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">pt</span><span class="o">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float64&quot;</span><span class="p">)]</span>
        <span class="k">return</span> <span class="n">Apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">perform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
        <span class="o">*</span><span class="n">flat_params</span><span class="p">,</span> <span class="n">images</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">jitted_cnn_op_jax</span><span class="p">(</span><span class="n">flat_params</span><span class="p">,</span> <span class="n">images</span><span class="p">)</span>
        <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float64&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">output_gradients</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;PyTensor gradient of CNNOp not implemented&quot;</span><span class="p">)</span>


<span class="nd">@jax_funcify</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">CNNOp</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">cnn_op_jax_funcify</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">perform</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
        <span class="o">*</span><span class="n">flat_params</span><span class="p">,</span> <span class="n">images</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="k">return</span> <span class="n">cnn_op_jax</span><span class="p">(</span><span class="n">flat_params</span><span class="p">,</span> <span class="n">images</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">perform</span>

<span class="n">cnn_op</span> <span class="o">=</span> <span class="n">CNNOp</span><span class="p">()</span>
</pre></div>
<p>We can now create a Bayesian Neural Network model,
giving a Normal prior for all the parameters in the CNN.</p>
<div class="hll"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">weights_prior</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">layer_name</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">params</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">layer_weights_name</span><span class="p">,</span> <span class="n">layer_weights</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">items</span><span class="p">()):</span>
            <span class="n">prior_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">layer_name</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">layer_weights_name</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="n">layer_weights_prior</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">prior_name</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">layer_weights</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="n">weights_prior</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer_weights_prior</span><span class="p">)</span>

    <span class="n">logitp_classes</span> <span class="o">=</span> <span class="n">cnn_op</span><span class="p">(</span><span class="o">*</span><span class="n">weights_prior</span><span class="p">,</span> <span class="n">train</span><span class="p">[</span><span class="s2">&quot;image&quot;</span><span class="p">])</span>
    <span class="n">logitp_classes</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">specify_shape</span><span class="p">(</span><span class="n">logitp_classes</span><span class="p">,</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">]),</span> <span class="mi">10</span><span class="p">))</span>
    <span class="n">label</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="s2">&quot;label&quot;</span><span class="p">,</span> <span class="n">logit_p</span><span class="o">=</span><span class="n">logitp_classes</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">train</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">])</span>

<span class="n">pm</span><span class="o">.</span><span class="n">model_to_graphviz</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
<p><img src="output_58_0.svg" alt="svg"></p>
<p>As before, we can compute the logp at the models' initial point,
which lets us figure out whether there are any issues with the model or not.</p>
<div class="hll"><pre><span></span><span class="c1"># Compute the logp at the initial point</span>
<span class="n">ip</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">initial_point</span><span class="p">()</span>
<span class="n">logp_fn</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">compile_fn</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">logp</span><span class="p">(</span><span class="nb">sum</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="n">logp_fn</span><span class="p">(</span><span class="n">ip</span><span class="p">)</span>
</pre></div>
<div class="hll"><pre><span></span><span class="n">array</span><span class="p">(</span><span class="o">-</span><span class="mf">759928.81029946</span><span class="p">)</span>
</pre></div>
<p>We can do the same with the JAX backend.</p>
<div class="hll"><pre><span></span><span class="c1"># Same in JAX backend</span>
<span class="n">logp_fn</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">compile_fn</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">logp</span><span class="p">(</span><span class="nb">sum</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;JAX&quot;</span><span class="p">)</span>
<span class="n">logp_fn</span><span class="p">(</span><span class="n">ip</span><span class="p">)</span>
</pre></div>
<div class="hll"><pre><span></span><span class="n">DeviceArray</span><span class="p">(</span><span class="o">-</span><span class="mf">759928.81030185</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float64</span><span class="p">)</span>
</pre></div>
<p>As we mentioned, we don't always need to define the gradient method.
For instance, when using JAX samplers such as <code>sample_numpyro_nuts</code>,
the gradients will be directly obtained from the jax compiled function.</p>
<p>Let's confirm this is the case,
by using the PyMC helper <code>get_jaxified_logp</code>
that returns the JAX function that computes the model joint logp,
and then taking the gradient with respect to the first set of parameters.
Firstly, we use the <code>get_jaxified_logp</code> helper to get the JAX function
(and we evaluate it below):</p>
<div class="hll"><pre><span></span><span class="kn">from</span> <span class="nn">pymc.sampling.jax</span> <span class="kn">import</span> <span class="n">get_jaxified_logp</span>

<span class="n">logp_fn</span> <span class="o">=</span> <span class="n">get_jaxified_logp</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">logp_fn</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">ip</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span>
</pre></div>
<div class="hll"><pre><span></span><span class="n">DeviceArray</span><span class="p">(</span><span class="o">-</span><span class="mf">759928.81030185</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float64</span><span class="p">)</span>
</pre></div>
<p>And then, we take the gradient with respect to the first set of parameters
and evaluate it below as well:</p>
<div class="hll"><pre><span></span><span class="n">dlogp_fn</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">logp_fn</span><span class="p">)</span>
<span class="n">dlogp_fn</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">ip</span><span class="o">.</span><span class="n">values</span><span class="p">()))[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
<div class="hll"><pre><span></span><span class="n">DeviceArray</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span>
             <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span>
             <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float64</span><span class="p">)</span>
</pre></div>
<h2 id="summary">Summary</h2><p>We hope you found this introduction to using PyMC with JAX helpful.
JAX is a powerful automatic differentiation library,
and a growing ecosystem is forming around it.
PyTensor is a flexible library for the compilation and manipulation of symbolic expressions,
for which JAX is one supported backend.
We hope that this introduction will help you to use JAX with PyMC,
and that you will find it helpful in your work!</p>

	<!--THIS IS THE FOOTER OF THE BLOGPSOT-->
	<hr> 
		<!--div class="container"-->
			<h2 class="font-roboto">Work with PyMC Labs</h2>
			<p>If you are interested in seeing what we at PyMC Labs can do for you, then please email <a href="mailto:info@pymc-labs.com">info@pymc-labs.com</a>. We work with companies at a variety of scales and with varying levels of existing modeling capacity.

We also run <a href="https://www.pymc-labs.com/workshops/">corporate workshop training events</a> and can provide sessions ranging from introduction to Bayes to more advanced topics.
			</p>
		<!--/div-->
    
    </div>
    <div class="col-md-2"></div>
</div>


        </div>

        <!-- Optional JavaScript -->
        <!-- jQuery first, then Popper.js, then Bootstrap JS -->
        <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
            integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
            crossorigin="anonymous"></script>
        <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js"
            integrity="sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN"
            crossorigin="anonymous"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"
            integrity="sha384-B4gt1jrGC7Jh4AgTPSdUtOBvfO8shuf57BaghqFfPlYxofvL8/KUEfYiJOMMV+rV"
            crossorigin="anonymous"></script>
        <script src="https://kit.fontawesome.com/8cc267a9ab.js" crossorigin="anonymous"></script>

        <nav class="navbar navbar-expand-lg navbar-light bg-light fixed-bottom">
            <div class="container">
                <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarBottom"
                    aria-controls="navbarBottom" aria-expanded="false" aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon"></span>
                </button>
                <div class="collapse navbar-collapse" id="navbarBottom">
                    <ul class="navbar-nav ml-auto">
                        
                        <li class="nav-item">
                            <a class="nav-link" href="https://twitter.com/pymc_labs"><i class="fa fa-twitter"
                                    aria-hidden="true"></i>
                                Twitter</a>
                        </li>
                        
                        <li class="nav-item">
                            <a class="nav-link" href="https://github.com/pymc-labs"><i class="fa fa-github"
                                    aria-hidden="true"></i>
                                GitHub</a>
                        </li>
                        
                        <li class="nav-item">
                            <a class="nav-link" href="https://www.linkedin.com/company/pymc-labs/"><i class="fa fa-linkedin"
                                    aria-hidden="true"></i>
                                LinkedIn</a>
                        </li>
                        
                        <li class="nav-item">
                            <a class="nav-link" href="https://www.youtube.com/c/PyMCLabs"><i class="fa fa-youtube"
                                    aria-hidden="true"></i>
                                YouTube</a>
                        </li>
                        
                        <li class="nav-item">
                            <a class="nav-link" href="https://www.meetup.com/pymc-labs-online-meetup/"><i class="fa fa-meetup"
                                    aria-hidden="true"></i>
                                Meetup</a>
                        </li>
                        
                        <li class="nav-item">
                            <a class="nav-link" href="/newsletter"><i class="fa fa-solid fa-bell"
                                    aria-hidden="true"></i>
                                Newsletter</a>
                        </li>
                        
                        <li class="nav-item">
                            <a class="nav-link" href="/privacy-policy"><i class="fa fa-solid fa-lock"
                                    aria-hidden="true"></i>
                                Privacy Policy</a>
                        </li>
                        
                        <li class="nav-item">
                            <a class="nav-link" href="/impressum"><i class="fa fa-solid fa-info-circle"
                                    aria-hidden="true"></i>
                                Impressum</a>
                        </li>
                        
                        <li class="nav-item">
                            <a class="nav-link" href="/contact"><i class="fa fa-solid fa-file-signature"
                                    aria-hidden="true"></i>
                                Contact</a>
                        </li>
                        
                    </ul>
                </div>
            </div>
        </nav>

    <!-- Mathjax for latex/equations -->
    <!-- Mathjax -->
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
        </script>

    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']]}});
    </script>

    </body>

</html>
